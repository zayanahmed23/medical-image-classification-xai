# Explainable AI for Medical Image Classification

This repository investigates post-hoc explainability methods for deep learning models used in medical image classification,
with a focus on reliability and faithfulness of explanations in clinical contexts.

## Research Objectives
- Train a CNN-based medical image classifier
- Apply multiple explainability techniques (Grad-CAM, Integrated Gradients, Occlusion)
- Analyze the faithfulness and limitations of explanation methods
- Identify failure cases and misleading explanations

## Dataset
Chest X-ray Pneumonia dataset (binary classification).

The dataset is not included in this repository. See `data/README.md` for download instructions.

## Explainability Methods (Planned)
- Grad-CAM / Grad-CAM++
- Integrated Gradients
- Occlusion-based perturbation analysis

## Status
Active research prototype (ongoing).
This will be edited overtime as my project pushes to completion.
